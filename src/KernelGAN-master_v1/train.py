import os
import tqdm
from itertools import product
from typing import Iterable, Any, Dict
import argparse
from .configs import Config
from .data import DataGenerator
from .kernelGAN import KernelGAN
from .learner import Learner
from .util import save_plot, save_conf
from os.path import exists, join, basename
from os import remove, listdir, getcwd
from PIL import Image
from datetime import datetime
import numpy as np
import random
import torch
from src.visualization.plot_utils import inference_plot_and_save, calc_avg_metrics
from .noise_estimation import noise_estimate
import matplotlib.pyplot as plt


def train(conf):
    gan = KernelGAN(conf)
    learner = Learner()
    data = DataGenerator(conf, gan)
    for iteration in range(
        conf.max_iters
    ):  # tqdm.tqdm(range(conf.max_iters), ncols=60):
        [g_in, d_in] = data.__getitem__(iteration)
        gan.train(g_in, d_in)
        learner.update(iteration, gan)
        # gan.plot()

        # if iteration % 500 == 0:
        #     gan.checkpoint(iteration)
    # save_plot(conf)

    save_conf(conf)
    save_path = gan.finish(iteration)
    return save_path


def create_params(lrfile, args):
    params = [
        "--input_image_path",
        lrfile,
        "--output_dir_path",
        os.path.abspath(args.output_dir),
        "--noise_scale",
        str(args.noise_scale),
        # "--D_n_layers",
        # str(args.D_n_layers),
    ]

    if args.X4:
        params.append("--X4")
    if args.SR:
        params.append("--do_ZSSR")
    if args.real:
        params.append("--real_image")
    return params


def grid_parameters(parameters: Dict[str, Iterable[Any]]) -> Iterable[Dict[str, Any]]:

    # grid generated by the product keeps the order of values
    for params in product(*parameters.values()):
        x = []
        for i in range(len(params)):
            x.append(list(parameters.keys())[i])
            x.append(str(params[i]))

        yield x


def is_image_file(filename):
    return any(filename.endswith(extension) for extension in [".png", ".jpg", ".jpeg"])


def main(args_range_dict):
    """The main function - performs kernel estimation (+ ZSSR) for all images in the 'test_images' folder"""
    set_seed()
    # Parse the command line arguments
    prog = argparse.ArgumentParser()
    prog.add_argument(
        "--paired-data-dir",
        "-pd",
        type=str,
        default="./src/KernelGAN-master_v1/val_fdata",
        help="path to image input directory.",
    )
    prog.add_argument(
        "--input-dir",
        "-i",
        type=str,
        default=os.getcwd()
        + "/src/KernelGAN-master_v1/val_fdata/input_lr",  # _mixed_small/input_lr", # "/data/val_mixed/input_lr/", #"/src/KernelGAN-master_v1/val_mixed_large/input_lr",
        help="path to image input directory.",
    )
    prog.add_argument(
        "--output-dir",
        "-o",
        type=str,
        default=os.getcwd()
        + "/results/kernelgan/results/"
        + "kernelgan"
        + datetime.now().strftime("_%b_%d_%H_%M_%S_%f")
        + "/",
        help="path to image output directory.",
    )
    prog.add_argument("--X4", action="store_true", help="The wanted SR scale factor")
    prog.add_argument(
        "--SR", action="store_true", help="when activated - ZSSR is not performed"
    )
    prog.add_argument(
        "--real", action="store_true", help="ZSSRs configuration is for real images"
    )
    prog.add_argument(
        "--noise_scale",
        type=float,
        default=1.0,
        help="ZSSR uses this to partially de-noise images",
    )

    # reading file cli args
    args_main = prog.parse_args()

    args_main.lr_dir = join(
        args_main.paired_data_dir, "input_lr"
    )  # paired_data_dir.join("input_lr")
    args_main.hr_dir = join(args_main.paired_data_dir, "output_hr")
    args_main.output_filenames = [
        join(args_main.hr_dir, x) for x in listdir(args_main.hr_dir) if is_image_file(x)
    ]
    args_main.input_filenames = [
        join(args_main.lr_dir, x) for x in listdir(args_main.lr_dir) if is_image_file(x)
    ]
    args_main.output_filenames.sort()
    args_main.input_filenames.sort()

    args_main.output_dir = os.path.join(
        args_main.output_dir, datetime.now().strftime("_%b_%d_%H_%M_%S_%f")
    )
    bicubic_metrics_list = []
    output_metrics_list = []
    # Run the KernelGAN sequentially on all images in the input directory
    for lrfile, hrfile in zip(args_main.input_filenames, args_main.output_filenames):

        for x in grid_parameters(args_range_dict):
            args = create_params(lrfile, args_main)
            # x is a list similar to return of create params
            args.extend(x)
            conf = Config().parse(args)
            conf.noise_scale = noise_estimate(plt.imread(lrfile), 4)

            print(f"_________noise={conf.noise_scale}___")
            save_path = train(conf)
            a, b = save_results(lrfile, hrfile, save_path)
            bicubic_metrics_list.append(a)
            output_metrics_list.append(b)
    psnr_bicubic, ssim_bicubic, lpips_distance_bicubic = calc_avg_metrics(
        bicubic_metrics_list
    )
    psnr_output, ssim_output, lpips_distance_output = calc_avg_metrics(
        output_metrics_list
    )
    print(
        f"psnr_bicubic {psnr_bicubic}, ssim_bicubic {ssim_bicubic}, lpips_distance_bicubic {lpips_distance_bicubic}"
    )
    print(
        f"psnr_output {psnr_output}, ssim_output {ssim_output}, lpips_distance_output {lpips_distance_output}"
    )

    prog.exit(0)


def save_results(lrfile, gtfile, hrfile):
    lr_img = Image.open(lrfile).convert("RGB")
    hr_img = Image.open(hrfile).convert("RGB")
    gt_img = Image.open(gtfile).convert("RGB")
    output_dir = os.path.split(hrfile)[0]
    output_name = "/" + os.path.split(hrfile)[1][:-3]
    bicubic_metrics, output_metrics = inference_plot_and_save(
        gt_img, lr_img, hr_img, output_dir, output_name
    )
    return bicubic_metrics, output_metrics


def set_seed(seed: int = 23) -> None:
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    # When running on the CuDNN backend, two further options must be set
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # Set a fixed value for the hash seed
    os.environ["PYTHONHASHSEED"] = str(seed)
    print(f"Random seed set as {seed}")


if __name__ == "__main__":
    #    args_range_dict = {"--D_n_layers": [7, 9, 13], "--D_kernel_size": [11, 13]}
    #    args_range_dict = {"--D_n_layers": [7, 9, 13], "--noise_scale" : [0.2,0.5,1.0]}
    args_range_dict = {"--max_iters": [3000]}
    main(args_range_dict)
